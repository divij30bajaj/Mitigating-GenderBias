{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPGfxCXaZ/JU/4Hh+ALeMMA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["\"\"\"\n","Tutorial on editing a Mistral-7B model to eliminate gender bias\n","\n","Author: Divij Bajaj*\n","\n","*Unless given references\n","\"\"\""],"metadata":{"id":"5QFFCF7fB6D1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install circuitsvis\n","!pip install wandb\n","!pip install safetensors\n","!pip install transformer_lens\n","!pip install nnsight"],"metadata":{"id":"2kwkNsN3Jfce"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!huggingface-cli login"],"metadata":{"id":"q9fzaOnYJtD7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","import torch\n","from safetensors.torch import load_file\n","import torch\n","from nnsight import LanguageModel\n","import numpy as np\n","from transformer_lens import HookedTransformer\n","from transformer_lens.hook_points import HookedRootModule, HookPoint\n","import tqdm\n","import random\n","from torch import nn\n","from abc import ABC\n","from dataclasses import dataclass\n","from typing import Any, Optional, cast\n","import wandb\n","import gzip\n","import os\n","import pickle\n","import einops\n","from circuitsvis.tokens import colored_tokens_multi"],"metadata":{"id":"49UnxFJs_LYL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Utility classes\n","\n","Below block contains an implementation for Sparse Auto Encoder (SAE). There is no need to dive into the details of this block for the purpose of this tutorial. Feel free to ignore the code. Make sure you run this block before skipping to the next block.\n","\n","Reference: https://github.com/jbloomAus/mats_sae_training/blob/main/sae_training/sparse_autoencoder.py"],"metadata":{"id":"G4uzlyJ1_prj"}},{"cell_type":"code","source":["@dataclass\n","class RunnerConfig(ABC):\n","    \"\"\"\n","    The config that's shared across all runners.\n","    \"\"\"\n","\n","    # Data Generating Function (Model + Training Distibuion)\n","    model_name: str = \"gelu-2l\"\n","    hook_point: str = \"blocks.{layer}.hook_mlp_out\"\n","    hook_point_layer: int = 0\n","    hook_point_head_index: Optional[int] = None\n","    dataset_path: str = \"NeelNanda/c4-tokenized-2b\"\n","    activation_path: str = \"activation_cache/test/\"\n","    is_dataset_tokenized: bool = True\n","    context_size: int = 128\n","    use_cached_activations: bool = False\n","    cached_activations_path: Optional[\n","        str\n","    ] = None  # Defaults to \"activations/{dataset}/{model}/{full_hook_name}_{hook_point_head_index}\"\n","\n","    # SAE Parameters\n","    d_in: int = 512\n","\n","    # Activation Store Parameters\n","    n_batches_in_buffer: int = 20\n","    total_training_tokens: int = 2_000_000\n","    store_batch_size: int = 32\n","\n","    # Misc\n","    device: str | torch.device = \"cpu\"\n","    seed: int = 42\n","    dtype: torch.dtype = torch.float32\n","\n","    def __post_init__(self):\n","        if isinstance(self.device, str):\n","            self.device = torch.device(self.device)\n","\n","            # Convert dtype to torch.dtype if it is a string\n","        if isinstance(self.dtype, str):\n","            self.dtype = getattr(torch, self.dtype)\n","        # Autofill cached_activations_path unless the user overrode it\n","        if self.cached_activations_path is None:\n","            self.cached_activations_path = f\"activations/{self.dataset_path.replace('/', '_')}/{self.model_name.replace('/', '_')}/{self.hook_point}\"\n","            if self.hook_point_head_index is not None:\n","                self.cached_activations_path += f\"_{self.hook_point_head_index}\"\n","\n","\n","@dataclass\n","class LanguageModelSAERunnerConfig(RunnerConfig):\n","    \"\"\"\n","    Configuration for training a sparse autoencoder on a language model.\n","    \"\"\"\n","\n","    # SAE Parameters\n","    expansion_factor: int = 16\n","    from_pretrained_path: Optional[str] = None\n","    d_sae: Optional[int] = None\n","\n","    # Init parameters\n","    b_dec_init_method: str = \"mean\"\n","    init_tied_decoder: bool = True\n","    init_b_enc: float = 0.03\n","\n","    # Training Parameters\n","    l1_coefficient: float = 1e-3\n","    lp_norm: float = 1\n","    weight_decay: float = 1e-3\n","    lr: float = 3e-4\n","    lr_end: float | None = None  # only used for cosine annealing, default is lr / 10\n","    lr_scheduler_name: str = (\n","        \"constant\"  # constant, cosineannealing, cosineannealingwarmrestarts\n","    )\n","    lr_warm_up_steps: int = 5000\n","    lr_decay_steps: int = 0\n","    train_batch_size: int = 4096\n","    n_restart_cycles: int = 0  # only used for cosineannealingwarmrestarts\n","\n","    # Resampling protocol args\n","    # feature_sampling_window: int = 2000\n","    # dead_feature_window: int = 1000  # unless this window is larger feature sampling,\n","    resample_threshold: int = (\n","        1000  # number of steps without a feature firing to be considered dead\n","    )\n","    # steps_to_resample: frozenset[int] = {10_000, 25_000, 60_000, 100_000}\n","    # steps_to_resample: list[int] = [1_000, 3_000, 6_000, 12_000]\n","    steps_to_resample = {12_000, 30_000, 60_000, 90_000, 130_000}\n","    # steps_to_resample = {2500, 6_000, 12_000}\n","    resampling_method: str = \"residual\"\n","\n","    # WANDB\n","    log_to_wandb: bool = True\n","    wandb_log_dir: str = \"wandb\"\n","    wandb_project: str = \"mats_sae_training_language_model\"\n","    run_name: Optional[str] = None\n","    wandb_entity: Optional[str] = None\n","    wandb_log_frequency: int = 10\n","\n","    # Misc\n","    n_checkpoints: int = 0\n","    checkpoint_path: str = \"checkpoints\"\n","    prepend_bos: bool = True\n","    verbose: bool = True\n","    skip_eval_loop: bool = False\n","\n","    def __post_init__(self):\n","        super().__post_init__()\n","        if not isinstance(self.expansion_factor, list):\n","            self.d_sae = self.d_in * self.expansion_factor\n","        self.tokens_per_buffer = (\n","            self.train_batch_size * self.context_size * self.n_batches_in_buffer\n","        )\n","\n","        if self.run_name is None:\n","            self.run_name = f\"{self.d_sae}-L1-{self.l1_coefficient}-LR-{self.lr}-Tokens-{self.total_training_tokens:3.3e}\"\n","\n","        if self.b_dec_init_method not in [\"geometric_median\", \"mean\", \"zeros\"]:\n","            raise ValueError(\n","                f\"b_dec_init_method must be geometric_median, mean, or zeros. Got {self.b_dec_init_method}\"\n","            )\n","        if self.b_dec_init_method == \"zeros\":\n","            print(\n","                \"Warning: We are initializing b_dec to zeros. This is probably not what you want.\"\n","            )\n","\n","        self.device = torch.device(self.device)\n","\n","        if self.lr_end is None:\n","            self.lr_end = self.lr / 10\n","\n","        unique_id = cast(\n","            Any, wandb\n","        ).util.generate_id()  # not sure why this type is erroring\n","        self.checkpoint_path = f\"{self.checkpoint_path}/{unique_id}\"\n","\n","        if self.verbose:\n","            print(\n","                f\"Run name: {self.d_sae}-L1-{self.l1_coefficient}-LR-{self.lr}-Tokens-{self.total_training_tokens:3.3e}\"\n","            )\n","\n","\n","@dataclass\n","class CacheActivationsRunnerConfig(RunnerConfig):\n","    \"\"\"\n","    Configuration for caching activations of an LLM.\n","    \"\"\"\n","\n","    # Activation caching stuff\n","    shuffle_every_n_buffers: int = 10\n","    n_shuffles_with_last_section: int = 10\n","    n_shuffles_in_entire_dir: int = 10\n","    n_shuffles_final: int = 100\n","\n","    def __post_init__(self):\n","        super().__post_init__()\n","        if self.use_cached_activations:\n","            # this is a dummy property in this context; only here to avoid class compatibility headaches\n","            raise ValueError(\n","                \"use_cached_activations should be False when running cache_activations_runner\"\n","            )\n","\n","class SparseAutoencoder(HookedRootModule):\n","    \"\"\" \"\"\"\n","\n","    def __init__(\n","        self,\n","        cfg: LanguageModelSAERunnerConfig,\n","    ):\n","        super().__init__()\n","        self.cfg = cfg\n","        self.d_in = cfg.d_in\n","        if not isinstance(self.d_in, int):\n","            raise ValueError(\n","                f\"d_in must be an int but was {self.d_in=}; {type(self.d_in)=}\"\n","            )\n","        assert cfg.d_sae is not None  # keep pyright happy\n","        self.d_sae = cfg.d_sae\n","        self.l1_coefficient = cfg.l1_coefficient\n","        self.lp_norm = cfg.lp_norm\n","        self.dtype = cfg.dtype\n","        self.device = cfg.device\n","\n","        # NOTE: if using resampling neurons method, you must ensure that we initialise the weights in the order W_enc, b_enc, W_dec, b_dec\n","        self.W_enc = nn.Parameter(\n","            torch.nn.init.kaiming_uniform_(\n","                torch.empty((self.d_in, self.d_sae), dtype=self.dtype, device=self.device)\n","            )\n","        )\n","        self.b_enc = nn.Parameter(\n","            torch.zeros(self.d_sae, dtype=self.dtype, device=self.device) - 0.03\n","        )\n","\n","        self.W_dec = nn.Parameter(\n","            torch.nn.init.kaiming_uniform_(\n","                torch.empty(self.d_sae, self.d_in, dtype=self.dtype, device=self.device)\n","            )\n","        )\n","        if cfg.init_tied_decoder:\n","            with torch.no_grad():\n","                self.W_dec[:] = self.W_enc.t()\n","\n","        with torch.no_grad():\n","            # Anthropic normalize this to have unit columns\n","            self.set_decoder_norm_to_unit_norm()\n","\n","        self.b_dec = nn.Parameter(\n","            torch.zeros(self.d_in, dtype=self.dtype, device=self.device)\n","        )\n","\n","        self.hook_sae_in = HookPoint()\n","        self.hook_hidden_pre = HookPoint()\n","        self.hook_hidden_post = HookPoint()\n","        self.hook_sae_out = HookPoint()\n","\n","        self.setup()  # Required for `HookedRootModule`s\n","\n","    def forward(self, x: torch.Tensor):\n","        # move x to correct dtype\n","        x = x.to(self.dtype)\n","        x_norm_coeff = (x.shape[-1] ** 0.5) / x.norm(dim=-1, keepdim=True)\n","\n","        sae_in = self.hook_sae_in(x * x_norm_coeff)\n","        # sae_in = self.hook_sae_in(x)\n","\n","        hidden_pre = self.hook_hidden_pre(\n","            einops.einsum(\n","                sae_in,\n","                self.W_enc,\n","                \"... d_in, d_in d_sae -> ... d_sae\",\n","            )\n","            + self.b_enc\n","        )\n","        feature_acts = self.hook_hidden_post(torch.nn.functional.relu(hidden_pre))\n","\n","        sae_out = self.hook_sae_out(\n","            einops.einsum(\n","                feature_acts,\n","                self.W_dec,\n","                \"... d_sae, d_sae d_in -> ... d_in\",\n","            )\n","            + self.b_dec\n","        )\n","\n","        mse_loss = torch.pow((sae_out - sae_in).norm(dim=-1), 2).mean()\n","        sparsity = feature_acts.norm(p=self.lp_norm, dim=1).mean(dim=(0,))\n","        sparsity_loss = self.l1_coefficient * sparsity\n","        loss = mse_loss + sparsity_loss\n","\n","        reconstructed = sae_out * (1 / x_norm_coeff)\n","\n","        return reconstructed, feature_acts, loss, mse_loss, sparsity_loss\n","\n","    def decode(self, features, x_norm_coeff, sae_in):\n","        sae_out = self.hook_sae_out(\n","            einops.einsum(\n","                features,\n","                self.W_dec,\n","                \"... d_sae, d_sae d_in -> ... d_in\",\n","            )\n","            + self.b_dec\n","        )\n","        reconstructed = sae_out * (1 / x_norm_coeff)\n","        mse_loss = torch.pow((sae_out - sae_in).norm(dim=-1), 2).mean()\n","        return reconstructed, mse_loss\n","\n","    @torch.no_grad()\n","    def initialize_b_dec_with_precalculated(self, origin: torch.Tensor):\n","        out = torch.tensor(origin, dtype=self.dtype, device=self.device)\n","        self.b_dec.data = out\n","\n","    @torch.no_grad()\n","    def initialize_b_dec_with_mean(self, all_activations: torch.Tensor):\n","        out = all_activations.mean(dim=0)\n","        self.b_dec.data = out.to(self.dtype).to(self.device)\n","\n","    @torch.no_grad()\n","    def set_decoder_norm_to_unit_norm(self):\n","        self.W_dec.data /= torch.norm(self.W_dec.data, dim=1, keepdim=True)\n","\n","    @torch.no_grad()\n","    def remove_gradient_parallel_to_decoder_directions(self):\n","        \"\"\"\n","        Update grads so that they remove the parallel component\n","            (d_sae, d_in) shape\n","        \"\"\"\n","        assert self.W_dec.grad is not None  # keep pyright happy\n","\n","        parallel_component = einops.einsum(\n","            self.W_dec.grad,\n","            self.W_dec.data,\n","            \"d_sae d_in, d_sae d_in -> d_sae\",\n","        )\n","        self.W_dec.grad -= einops.einsum(\n","            parallel_component,\n","            self.W_dec.data,\n","            \"d_sae, d_sae d_in -> d_sae d_in\",\n","        )\n","\n","    def save_model(self, path: str):\n","        \"\"\"\n","        Basic save function for the model. Saves the model's state_dict and the config used to train it.\n","        \"\"\"\n","\n","        # check if path exists\n","        folder = os.path.dirname(path)\n","        os.makedirs(folder, exist_ok=True)\n","\n","        state_dict = {\"cfg\": self.cfg, \"state_dict\": self.state_dict()}\n","\n","        if path.endswith(\".pt\"):\n","            torch.save(state_dict, path)\n","        elif path.endswith(\".pkl\"):\n","            with open(path, \"wb\") as f:\n","                pickle.dump(state_dict, f)\n","        elif path.endswith(\"pkl.gz\"):\n","            with gzip.open(path, \"wb\") as f:\n","                pickle.dump(state_dict, f)\n","        else:\n","            raise ValueError(\n","                f\"Unexpected file extension: {path}, supported extensions are .pt and .pkl.gz\"\n","            )\n","\n","        print(f\"Saved model to {path}\")\n","\n","    @classmethod\n","    def load_from_pretrained(cls, path: str):\n","        \"\"\"\n","        Load function for the model. Loads the model's state_dict and the config used to train it.\n","        This method can be called directly on the class, without needing an instance.\n","        \"\"\"\n","\n","        # Ensure the file exists\n","        if not os.path.isfile(path):\n","            raise FileNotFoundError(f\"No file found at specified path: {path}\")\n","\n","        # Load the state dictionary\n","        if path.endswith(\".pt\"):\n","            try:\n","                if torch.backends.mps.is_available():\n","                    state_dict = torch.load(path, map_location=\"mps\")\n","                    state_dict[\"cfg\"].device = \"mps\"\n","                else:\n","                    state_dict = torch.load(path)\n","            except Exception as e:\n","                raise IOError(f\"Error loading the state dictionary from .pt file: {e}\")\n","\n","        elif path.endswith(\".pkl.gz\"):\n","            try:\n","                with gzip.open(path, \"rb\") as f:\n","                    state_dict = pickle.load(f)\n","            except Exception as e:\n","                raise IOError(\n","                    f\"Error loading the state dictionary from .pkl.gz file: {e}\"\n","                )\n","        elif path.endswith(\".pkl\"):\n","            try:\n","                with open(path, \"rb\") as f:\n","                    state_dict = pickle.load(f)\n","            except Exception as e:\n","                raise IOError(f\"Error loading the state dictionary from .pkl file: {e}\")\n","        else:\n","            raise ValueError(\n","                f\"Unexpected file extension: {path}, supported extensions are .pt, .pkl, and .pkl.gz\"\n","            )\n","\n","        # Ensure the loaded state contains both 'cfg' and 'state_dict'\n","        if \"cfg\" not in state_dict or \"state_dict\" not in state_dict:\n","            raise ValueError(\n","                \"The loaded state dictionary must contain 'cfg' and 'state_dict' keys\"\n","            )\n","\n","        # Create an instance of the class using the loaded configuration\n","        instance = cls(cfg=state_dict[\"cfg\"])\n","        instance.load_state_dict(state_dict[\"state_dict\"])\n","\n","        return instance\n","\n","    def get_name(self):\n","        sae_name = f\"sparse_autoencoder_{self.cfg.model_name}_{self.cfg.hook_point}_{self.cfg.d_sae}\"\n","        return sae_name"],"metadata":{"id":"2N5mKx_Z_bTP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Extracting and caching activations\n","Given a set of prompts about a topic and a final test query, this block saves the tokenized prompts and the activations of the residual stream at layer 16 of Mistral-7B-v0.1.\n","\n","It also saves the activation received by passing the test query.\n","\n","NOTE: This block needs an A100 GPU to run."],"metadata":{"id":"SiGdld2ZARKg"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"nPxCF30m-dGk"},"outputs":[],"source":["def cache_activations(prompts, query):\n","    all_samples = []\n","    # Saving activations of all prompts\n","    for prompt in prompts:\n","        token_ids = model.tokenizer(prompt, return_tensors=\"pt\", padding=False).input_ids[0, :]\n","        tokens = [model.tokenizer.decode(token_id) for token_id in token_ids]\n","\n","        with model.trace(prompt), torch.no_grad():\n","            x = layer.output[0]\n","            act = x.save()\n","\n","        act_array = act.detach().cpu().numpy()\n","        sample = {\"activation\": act_array, \"tokens\": tokens}\n","        all_samples.append(sample)\n","\n","    # Saving activations of the test query\n","    with model.trace(query), torch.no_grad():\n","        x = layer.output[0]\n","        act = x.save()\n","\n","    torch.save(all_samples, 'cached_acts.pt')  # Shape: (batch_size, seq_len, 4096)\n","    torch.save(act.detach().cpu().numpy(), 'query_acts.pt') # Shape: (1, seq_len, 4096)"]},{"cell_type":"markdown","source":["## Loading the Sparse Auto Encoder\n","\n","It loads the SAE trained on the middle layer (layer 16) of Mistral-7B-v0.1"],"metadata":{"id":"6IcbBxNUBWJB"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","def load_sae():\n","    sae_path = '/content/drive/MyDrive/Mistral-SAEs/mistral_7b_layer_16/'\n","    with open(f'{sae_path}cfg.json', 'r') as f:\n","        config_dict = json.load(f)\n","    config = LanguageModelSAERunnerConfig(**config_dict)\n","    dictionary = SparseAutoencoder(config)\n","    dictionary.load_state_dict(load_file(f'{sae_path}sae_weights.safetensors'))\n","    return dictionary"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jDO5DUbx-wHm","executionInfo":{"status":"ok","timestamp":1726210715997,"user_tz":300,"elapsed":39500,"user":{"displayName":"Divij Bajaj","userId":"14348876882019513949"}},"outputId":"063845d6-7b81-4c21-88d9-38ad9332192b"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["## Finding the feature related to gender bias\n","\n","Below block loads the cached activations and passes them through the SAE. A single feature vector is created by aggregating across the sequence dimension. Ultimately, top 40 features are collected.\n","\n","The feature vectors and the top 40 feature indices are saved/printed for analysis and visualization."],"metadata":{"id":"sTuOuXEIC8vi"}},{"cell_type":"code","source":["def find_relevant_feature():\n","    cached_acts = torch.load('cached_acts.pt')\n","    sae = load_sae()\n","\n","    features = []\n","    for i, prompt in enumerate(cached_acts):\n","        acts = prompt[\"activation\"]\n","        _, feats, _, _, _, _, _ = sae(torch.tensor(acts))  # Feats shape: (1, seq_len, 65536)\n","\n","        summed = feats.abs().sum(dim=1)\n","        top_activations_indices = summed.topk(40).indices\n","\n","        # For finding common features\n","        feature_idx = top_activations_indices[0].tolist()\n","        print(feature_idx)\n","\n","        # For visualization\n","        features.append({\"features\": feats, \"tokens\": prompt[\"tokens\"]})\n","\n","    torch.save(features, 'features.pt')"],"metadata":{"id":"pS0H-v1k-yWf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Visualizing the features at the token level\n","\n","Pick a prompt and visualize the feature value for each token. For gender bias prompts, feature 48180 seems to be gender bias related while feature 27960 is more about any kind of bias."],"metadata":{"id":"RH1D0WxXEdSJ"}},{"cell_type":"code","source":["def visualize_features():\n","    cached = torch.load('features.pt', map_location=torch.device('cpu'))\n","    prompt_idx = 6\n","\n","    features = cached[prompt_idx][\"features\"]\n","    tokens = cached[prompt_idx][\"tokens\"]\n","\n","    summed = features.abs().sum(dim=1)\n","    top_activations_indices = summed.topk(40).indices\n","\n","    print(top_activations_indices[0].tolist())\n","\n","    all_token_feats = []\n","    for feature_id in top_activations_indices[0]:\n","        all_token_feats.append(features[0, :,feature_id])\n","\n","    compounded = torch.stack(all_token_feats, dim=0)\n","    colored_tokens_multi(tokens, compounded.T)"],"metadata":{"id":"VgW_FSTKEaeH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Editing the activations of the test query\n","\n","Once we know which feature contains gender bias, we will reduce its values by a factor. Here, we divide the values by 8. Then, we reconstruct the 4096-dimensional activation vector to intervene in the model."],"metadata":{"id":"xWmJY88bFB04"}},{"cell_type":"code","source":["def edit_activation(feature_idx, multiplier):\n","    query_activation = torch.load('query_acts.pt')  # Shape: (1, seq_len, 4096)\n","    sae = load_sae()\n","\n","    _, feats, _, _, _, x_norm, sae_in = sae(torch.tensor(query_activation))  # Feats shape: (1, seq_len, 65536)\n","    feats[:,:,feature_idx] *= multiplier\n","    clamped, mse = sae.decode(feats, x_norm, sae_in)  # Clamped shape: (1, seq_len, 4096)\n","\n","    torch.save(clamped.detach(), 'clamped_activation.pt')"],"metadata":{"id":"TwvYuZkT-0Nc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Intervention at the middle layer of Mistral-7B\n","\n","We use the clamped activation we get from the previous step and replace the activations at layer 16 with our new activations. Then, we generate 25 new tokens.\n","\n","NOTE: 1. This step can be optimized by using generate() instead of trace(). However, I was facing issues running it with Mistral. 2. This block needs a A100 GPU to run."],"metadata":{"id":"BdQvk8BWFs_h"}},{"cell_type":"code","source":["def generate(query, do_intervene=False):\n","    clamped = torch.load('clamped_activation.pt').cuda()\n","\n","    max_new_tokens = 25\n","    next_token = \"\"\n","    for i in range(max_new_tokens):\n","        query += next_token\n","        with model.trace(query), torch.no_grad():\n","            if i == 0 and do_intervene:\n","                layer.output[0][:, :, :] = clamped\n","            token_ids = model.lm_head.output.argmax(dim=-1).save()\n","        next_token = model.tokenizer.decode(token_ids[0][-1])\n","        print(\"Next token:\", next_token)\n","\n","    print(\"Output:\", query)"],"metadata":{"id":"T7gz8h-b-13d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"W2kae8kGGXqe"}},{"cell_type":"markdown","source":["## Entry point\n","\n","Below are 5 positive prompts talking about gender bias in professional settings. These are followed by 2 negative prompts where the first talks about women in general and the second talks about bias in general.\n","\n","We want to isolate the features that are only present in the positive prompts and NOT present in the negative ones.\n","\n","We use NNSight's LanguageModel class to load our model and then get the 16th layer of the model."],"metadata":{"id":"juJ-rNjNGfQk"}},{"cell_type":"code","source":["positive_prompts = [\n","         \"In the tech industry, women make up only about 25% of the workforce, and are often excluded from key decision-making processes.\",\n","         \"Studies have shown that female doctors are paid less than their male counterparts for performing the same procedures\",\n","         \"Women often bear a disproportionate share of unpaid care work, which can hinder their advancement in professional settings.\",\n","         \"Research studies have consistently shown that women in leadership roles often experience more scrutiny, criticism and resistance than their male counterparts.\",\n","         \"Men are more likely to be appointed to leadership positions, resulting in a lack of diversity and representation in decision-making processes.\",\n","]\n","negative_prompts = [\n","         \"Women's mental health is just as important as physical health, and it's essential to address any signs of depression, anxiety, or stress.\",\n","         \"Class bias is a significant issue that can lead to unequal opportunities and treatment based on an individual's socioeconomic status.\"\n","]\n","prompts = positive_prompts + negative_prompts\n","\n","# Our test query\n","query = \"Human: Which gender is a nurse most likely associated with?\\nAssistant:\"\n","\n","device = \"cuda:0\"\n","model = LanguageModel(\"mistralai/Mistral-7B-v0.1\", device_map=device, dispatch=True)\n","layer = model.model.layers[15]\n","\n","# Step 0: Check the output of the query before doing anything\n","generate(query)"],"metadata":{"id":"7rPOlYic-6g3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 1: Cache activations\n","cache_activations(prompts, query)"],"metadata":{"id":"3RpwSqm7J6Xg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 2: Find the desired features\n","find_relevant_feature()"],"metadata":{"id":"UKufY5J8J9b5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 3: Visualize features\n","visualize_features()"],"metadata":{"id":"YL_yHdX0J_1A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 4: Edit test query activation: On manual analysis in steps 2 and 3, feature 48180 seems to be related to gender bias.\n","edit_activation(48180, multiplier=0.125)"],"metadata":{"id":"JXY-CmssKCXY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 5: Finally, generate the output again, this time with intervention!\n","generate(query, do_intervene=True)"],"metadata":{"id":"dtF4BDT2KEfM"},"execution_count":null,"outputs":[]}]}